{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Resources used to support this work:\n\n* paper: https://arxiv.org/pdf/2309.10952.pdf\n* model: https://huggingface.co/gpt2\n\n* https://www.kaggle.com/code/aliabdin1/llm-04a-fine-tuning-llms\n* https://huggingface.co/docs/transformers/tasks/language_modeling","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport pandas as pd\nfrom pydantic import BaseModel, root_validator\nfrom typing import Optional, Dict\nimport numpy as np\n\nimport json\n\nimport tempfile\nimport transformers as tr\nfrom difflib import SequenceMatcher\nfrom functools import partial\n\n\ndata_loc = \"/kaggle/input/sroie-datasetv2/SROIE2019/train\"\nos.environ['WANDB_DISABLED'] = \"True\"\n","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:27:54.000023Z","iopub.execute_input":"2023-11-06T13:27:54.000419Z","iopub.status.idle":"2023-11-06T13:27:54.359529Z","shell.execute_reply.started":"2023-11-06T13:27:54.000389Z","shell.execute_reply":"2023-11-06T13:27:54.358549Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"TOTAL\": {\"type\": \"string\"},\n        \"DATE\": {\"type\": \"string\"},\n        \"ADDRESS\": {\"type\": \"string\"},\n        \"COMPANY\": {\"type\": \"string\"},\n    },\n    \"required\": [\"TOTAL\",\"DATE\",\"ADDRESS\",\"COMPANY\"],\n}\n\nempty_schema = {\n    \"TOTAL\": \"\",\n   \"DATE\": \"\",\n   \"ADDRESS\": \"\",\n   \"COMPANY\": \"\",\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:27:54.368257Z","iopub.execute_input":"2023-11-06T13:27:54.368553Z","iopub.status.idle":"2023-11-06T13:27:54.376141Z","shell.execute_reply.started":"2023-11-06T13:27:54.368528Z","shell.execute_reply":"2023-11-06T13:27:54.375313Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Setting up model and saving temp files \n\ntmpdir = tempfile.TemporaryDirectory()\nlocal_training_root = tmpdir.name\n\nmodel_checkpoint = \"gpt2\"\n\ntokenizer = tr.AutoTokenizer.from_pretrained(\n    model_checkpoint, cache_dir=\"../working/cache\", additional_special_tokens = ['<Document>','</Document>','<Task>','</Task>','<Extraction>','</Extraction>']\n)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Loading model and resizing it for new tokens\ncheckpoint_name = \"test-trainer\"\nlocal_checkpoint_path = os.path.join(local_training_root, checkpoint_name)\nmodel = tr.AutoModelForCausalLM.from_pretrained(\n    model_checkpoint, cache_dir=\"../working/cache\"\n)\nmodel.resize_token_embeddings(len(tokenizer),pad_to_multiple_of=4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Methods to help load and pre-process data\n\ndef load_data_from_kaggle(data_loc):\n    bbox_loc = data_loc+'/box'\n    entities_loc = data_loc+'/entities'\n    \n    all_bboxes_files = os.listdir(bbox_loc)\n    all_bboxes = []\n    for bbox_file in all_bboxes_files:\n        with open(os.path.join(bbox_loc,bbox_file),'r') as f:\n            bbox = f.readlines()\n            all_bboxes.append(bbox)\n    \n    bboxes, words = process_bboxes(all_bboxes)\n        \n    all_entities_files = os.listdir(entities_loc)\n    all_entities = []\n    for entities_file in all_entities_files:\n        with open(os.path.join(entities_loc,entities_file),'r') as f:\n            entities = f.read()\n            all_entities.append(entities)\n            \n    file_ids = [f.replace('.txt','') for f in all_bboxes_files]\n    \n    return bboxes, words, all_entities, file_ids\n\n\ndef process_bboxes(all_bboxes):\n    bboxes = []\n    words = []\n    for doc in all_bboxes:\n        doc_bboxes = []\n        doc_words = []\n        for line in doc:\n            line = line.replace(r'\\n', '')\n            m = re.match(r\"(\\d*),(\\d*),(\\d*),(\\d*),(\\d*),(\\d*),(\\d*),(\\d*),(.*)\", line)\n            doc_bboxes.append(list(map(int, m.groups()[0:8])))\n            doc_words.append(m.groups()[8])\n        bboxes.append(doc_bboxes)\n        words.append(doc_words)\n    \n    return bboxes, words","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:27:54.377874Z","iopub.execute_input":"2023-11-06T13:27:54.378198Z","iopub.status.idle":"2023-11-06T13:27:54.385892Z","shell.execute_reply.started":"2023-11-06T13:27:54.378172Z","shell.execute_reply":"2023-11-06T13:27:54.385133Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Loading the data from kaggle, processing it and then saving a local json\n\nbboxes, words, entities, ids = load_data_from_kaggle(data_loc)\n\ndata_json = []\nfor bbox, word, entity, file_id in zip(bboxes, words, entities, ids):\n    data_json.append({'bbox':bbox, 'word':word, 'entity':json.loads(entity), 'key':file_id})\n    \nwith open('sorie-datasetv3.json','w') as f:\n    json.dump(data_json, f)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:27:54.396211Z","iopub.execute_input":"2023-11-06T13:27:54.396492Z","iopub.status.idle":"2023-11-06T13:27:55.133351Z","shell.execute_reply.started":"2023-11-06T13:27:54.396470Z","shell.execute_reply":"2023-11-06T13:27:55.132478Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Preprocessing util methods\n\ndef combine_bbox(bboxes):\n    numpy_bbox_matrix = np.array(bboxes)\n    return [\n        numpy_bbox_matrix[:,0].min(),\n        numpy_bbox_matrix[:, 1].min(),\n        numpy_bbox_matrix[:, 2].max(),\n        numpy_bbox_matrix[:, 3].max(),\n    ]\n\n\ndef convert_labels_entity(labels, words):\n    entities = {}\n    unique_labels = np.unique(labels)\n    label_array = np.array(labels)\n    word_array = np.array(words)\n    for label in unique_labels:\n        if label!=\"O\":\n            all_line = word_array[label_array==label]\n            label_value = ' '.join(all_line)\n            entities[label] = label_value\n    return entities\n\n\n\ndef get_bins_from_list(cord_list, bins=100):\n    return pd.cut(cord_list, bins=bins, labels=np.arange(1, bins+1)).to_list()\n\n\ndef spliting_criteria_len(text, tokenizer, max_length):\n    token_lengths = len(tokenizer(text)['input_ids'])\n    return token_lengths<=max_length\n\n\n\ndef assign_line_label(line: str, entity: dict):\n    line_set = line.replace(\",\", \"\").strip().split()\n    for column, value in entity.items():\n        entity_values = value.replace(\",\", \"\").strip()\n        entity_set = entity_values.split()\n        \n        \n        matches_count = 0\n        for l in line_set:\n            if any(SequenceMatcher(a=l, b=b).ratio() > 0.8 for b in entity_set):\n                matches_count += 1\n            \n            if (column.upper() == 'ADDRESS' and (matches_count / len(line_set)) >= 0.5) or \\\n               (column.upper() != 'ADDRESS' and (matches_count == len(line_set))) or \\\n               matches_count == len(entity_set):\n                return column.upper()\n\n    return \"O\"\n\n\ndef assign_labels(words: list, bboxes: list, entity: dict):\n    max_area = {\"TOTAL\": (0, -1), \"DATE\": (0, -1)}  # Value, index\n    already_labeled = {\"TOTAL\": False,\n                       \"DATE\": False,\n                       \"ADDRESS\": False,\n                       \"COMPANY\": False,\n                       \"O\": False\n    }\n\n    # Go through every line in $words and assign it a label\n    labels = []\n    for i, pair in enumerate(zip(words, bboxes)):\n        line, bbox = pair\n        label = assign_line_label(line=line, entity=entity)\n\n        already_labeled[label] = True\n        if (label == \"ADDRESS\" and already_labeled[\"TOTAL\"]) or \\\n           (label == \"COMPANY\" and (already_labeled[\"DATE\"] or already_labeled[\"TOTAL\"])):\n            label = \"O\"\n\n        # Assign to the largest bounding box\n        if label in [\"TOTAL\", \"DATE\"]:\n            area = (bbox[4] - bbox[0]) + (bbox[5] - bbox[1])\n\n            if max_area[label][0] < area:\n                max_area[label] = (area, i)\n\n            label = \"O\"\n\n        labels.append(label)\n\n    labels[max_area[\"DATE\"][1]] = \"DATE\"\n    labels[max_area[\"TOTAL\"][1]] = \"TOTAL\"\n    \n    return labels","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:27:55.517629Z","iopub.execute_input":"2023-11-06T13:27:55.517903Z","iopub.status.idle":"2023-11-06T13:27:55.524875Z","shell.execute_reply.started":"2023-11-06T13:27:55.517880Z","shell.execute_reply":"2023-11-06T13:27:55.523996Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Data models to store document level data and splits\n\nclass SorieDocument(BaseModel):\n    key: str\n    bbox: list\n    word: list\n    entity: Dict[str,str]\n    bbox_quantized: list\n    labels: list\n        \n    @root_validator(pre=True)\n    def compute_bbox_quantized(cls, values):\n        bbox_means = [[np.mean([b[0],b[2],b[4],b[6]]),np.mean([b[1],b[3],b[5],b[7]])] for b in values.get('bbox')]\n        values['bbox_quantized'] = np.apply_along_axis(get_bins_from_list, arr=bbox_means, axis=0).tolist()\n        return values\n    \n    @root_validator(pre=True)\n    def compute_labels_embeddings(cls, values):\n        values['labels']=assign_labels(words=values.get('word'), bboxes=values.get('bbox'), entity=values.get('entity'))\n        return values\n\n\nclass SplitSorieDocument(BaseModel):\n    key: str\n    index: int\n    bbox: list\n    word: list\n    bbox_quantized: list\n    labels: list\n    entity: dict\n        \n    @root_validator(pre=True)\n    def compute_entities(cls, values):\n        values['entity']=convert_labels_entity(labels=values.get('labels'), words=values.get('word'))\n        return values","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:27:55.588159Z","iopub.execute_input":"2023-11-06T13:27:55.588461Z","iopub.status.idle":"2023-11-06T13:27:55.600946Z","shell.execute_reply.started":"2023-11-06T13:27:55.588436Z","shell.execute_reply":"2023-11-06T13:27:55.600144Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Preprocess data for modelling. include schema genertion and data splitting\n\ndef create_string(txt, bbox):\n    return f\"{txt} {bbox[0]}|{bbox[1]}\"\n\n\ndef create_word_bbox_text(words, bboxes):\n    combined = [create_string(word, bbox) for bbox, word in zip(bboxes, words)]\n    combined_rows = \"\\n\".join(combined)\n    return f\"\"\"<Document>\\n{combined_rows}\\n</Document>\\n<Task>From the document, extract the text values and tags of the following entities:{json.dumps(empty_schema)}</Task>\\n<Extraction>\"\"\"\n\n\ndef split_doc_into_splits(doc, spliting_criteria):\n    split_docs = []\n    \n    words = doc.word\n    bboxes_quantized = doc.bbox_quantized\n\n    start_index = 0\n    split_index = 0\n    for index, _ in enumerate(words):\n        input_text = create_word_bbox_text(words[start_index:index+1], bboxes_quantized[start_index:index+1])\n        if index==len(words)-1:\n            split_docs.append(\n                SplitSorieDocument(\n                    word=words[start_index:index+1],\n                    bbox=doc.bbox[start_index:index+1],\n                    index=split_index,\n                    key=doc.key,\n                    bbox_quantized = bboxes_quantized[start_index:index+1],\n                    labels=doc.labels[start_index:index+1],\n                )\n            )\n            start_index = index\n            split_index+=1\n        else:\n            if spliting_criteria(input_text):\n                continue\n            else:\n                split_docs.append(\n                SplitSorieDocument(\n                    word=words[start_index:index],\n                    bbox=doc.bbox[start_index:index],\n                    index=split_index,\n                    key=doc.key,\n                    bbox_quantized = bboxes_quantized[start_index:index],\n                    labels=doc.labels[start_index:index],\n                )\n            )\n                start_index = index\n                split_index+=1\n\n    return split_docs","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:28:20.465998Z","iopub.execute_input":"2023-11-06T13:28:20.466770Z","iopub.status.idle":"2023-11-06T13:28:20.479035Z","shell.execute_reply.started":"2023-11-06T13:28:20.466723Z","shell.execute_reply":"2023-11-06T13:28:20.478017Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"all_docs = [SorieDocument.parse_obj(doc) for doc in data_json]\nall_splits = [split for doc in all_docs for split in split_doc_into_splits(doc, spliting_criteria=partial(spliting_criteria_len,tokenizer=tokenizer, max_length=256))]","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:31:40.037694Z","iopub.execute_input":"2023-11-06T13:31:40.038867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass SROIE_Dataset(Dataset):\n    def __init__(self, split_list, tokenizer):\n        self.split_list = split_list\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.split_list)\n    \n    def __getitem__(self,index):\n        input_text = create_word_bbox_text(self.split_list[index].word, self.split_list[index].bbox_quantized)\n        outputs = json.dumps(self.split_list[index].entity)+'</Extraction>'\n        encoding = tokenizer('\\n'.join([input_text,outputs]), return_tensors=\"pt\", truncation=True, padding=True,)\n        return {i:v[0] for i,v in encoding.items()}","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:28:41.945235Z","iopub.execute_input":"2023-11-06T13:28:41.945540Z","iopub.status.idle":"2023-11-06T13:28:41.952938Z","shell.execute_reply.started":"2023-11-06T13:28:41.945514Z","shell.execute_reply":"2023-11-06T13:28:41.951929Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"dataset = SROIE_Dataset(all_splits, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:28:41.954375Z","iopub.execute_input":"2023-11-06T13:28:41.954677Z","iopub.status.idle":"2023-11-06T13:28:41.962702Z","shell.execute_reply.started":"2023-11-06T13:28:41.954652Z","shell.execute_reply":"2023-11-06T13:28:41.961846Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"training_args = tr.TrainingArguments(\n    local_checkpoint_path,\n    num_train_epochs=3,  # default number of epochs to train is 3\n    per_device_train_batch_size=4,\n    logging_steps=25,\n    eval_steps= 50,\n    optim=\"adamw_torch\",\n    report_to=None,\n)\n\ndata_collator = tr.DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False)\n\ntrainer = tr.Trainer(\n    model,\n    training_args,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:28:46.043172Z","iopub.execute_input":"2023-11-06T13:28:46.043576Z","iopub.status.idle":"2023-11-06T13:28:49.006490Z","shell.execute_reply.started":"2023-11-06T13:28:46.043545Z","shell.execute_reply":"2023-11-06T13:28:49.005641Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:28:51.959188Z","iopub.execute_input":"2023-11-06T13:28:51.959980Z","iopub.status.idle":"2023-11-06T13:28:54.408884Z","shell.execute_reply.started":"2023-11-06T13:28:51.959937Z","shell.execute_reply":"2023-11-06T13:28:54.407317Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1813\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1810\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1813\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1814\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_utils.py:707\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[1;32m    706\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:732\u001b[0m, in \u001b[0;36mDataCollatorForLanguageModeling.torch_call\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtorch_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, examples: List[Union[List[\u001b[38;5;28mint\u001b[39m], Any, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# Handle dict or lists with proper padding and conversion to tensor.\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[0;32m--> 732\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    734\u001b[0m         batch \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    735\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: _torch_collate_batch(examples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_to_multiple_of)\n\u001b[1;32m    736\u001b[0m         }\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3018\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   3017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[0;32m-> 3018\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3021\u001b[0m     )\n\u001b[1;32m   3023\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n","\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []"],"ename":"ValueError","evalue":"You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []","output_type":"error"}]},{"cell_type":"code","source":"index = 11\nitem = tokenizer(create_word_bbox_text(all_splits[index].word,all_splits[index].bbox_quantized),return_tensors=\"pt\", truncation=True, padding=True,)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:29:15.290949Z","iopub.execute_input":"2023-11-06T13:29:15.291918Z","iopub.status.idle":"2023-11-06T13:29:15.297554Z","shell.execute_reply.started":"2023-11-06T13:29:15.291884Z","shell.execute_reply":"2023-11-06T13:29:15.296554Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"result_example = tokenizer.batch_decode(trainer.model.generate(input_ids=item[\"input_ids\"].to(\"cuda\"),attention_mask= item[\"attention_mask\"].to(\"cuda\"),max_new_tokens=100, do_sample=True, top_k=10, top_p=0.95), skip_special_tokens=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(create_word_bbox_text(all_splits[index].word,all_splits[index].bbox_quantized))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"json.dumps(all_splits[index].entity)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(result_example[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post Processing","metadata":{}},{"cell_type":"code","source":"import re\ndef find_results(text, schema):\n    reg_str = r\"<Extraction>(.*?)</Extraction>\"\n    text = text.replace(\"\\n\",\"\")\n    extracted_result = re.findall(reg_str, text)\n\n    json_extracted = []\n    for e in extracted_result:\n        try:\n            json_object = json.loads(e)\n            validate(json_object,schema)\n        except:\n            continue\n        json_extracted.append(json_object)\n    return json_extracted[0] if json_extracted else {}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = find_results(result_example[0], schema)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}